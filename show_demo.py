# -*- coding: utf-8 -*-
from transformers import CLIPProcessor, CLIPModel
import torch
import clip
from PIL import Image
import os
import shutil
import time
import gradio as gr
import numpy as np

# è®¾ç½®è®¾å¤‡
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"è¿è¡Œè®¾å¤‡: {device}")

# åŠ è½½CLIPæ¨¡å‹
model, preprocess = clip.load("ViT-B/32", device=device)

# ç¡®ä¿ä¸­æ–‡æ­£å¸¸æ˜¾ç¤º
import matplotlib.pyplot as plt
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]

# é…ç½®
output_image_folder = "images_onnx"
feature_library_path = "feature_library_onnx.pth"

# ç±»åˆ«æ ‡ç­¾ï¼ˆä¸­æ–‡ï¼‰
word_list = ["a fox", "a cat", " a crane", "a bird", "a rabbit", "a monkey", "a panda", "a elephant", "a lion", "a dog"]
texts = clip.tokenize(word_list).to(device)

# ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
if not os.path.exists(output_image_folder):
    os.makedirs(output_image_folder)


def load_feature_library():
    """åŠ è½½ç‰¹å¾åº“ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç©ºåº“"""
    if os.path.exists(feature_library_path):
        try:
            # å°è¯•åŠ è½½ç‰¹å¾åº“
            feature_library = torch.load(feature_library_path, weights_only=True).to(device)
            print(f"å·²åŠ è½½ç‰¹å¾åº“ï¼ŒåŒ…å« {len(feature_library)} ä¸ªç‰¹å¾")
            return feature_library
        except Exception as e:
            print(f"åŠ è½½ç‰¹å¾åº“å¤±è´¥: {e}")
            print("åˆ›å»ºæ–°çš„ç©ºç‰¹å¾åº“")
    return torch.empty((0, model.visual.output_dim), device=device)


def save_feature_library(feature_library):
    """ä¿å­˜ç‰¹å¾åº“"""
    try:
        torch.save(feature_library, feature_library_path)
        print(f"ç‰¹å¾åº“å·²ä¿å­˜ï¼ŒåŒ…å« {len(feature_library)} ä¸ªç‰¹å¾")
    except Exception as e:
        print(f"ä¿å­˜ç‰¹å¾åº“å¤±è´¥: {e}")


def classify_image(input_image_path):
    """å¯¹è¾“å…¥å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œå¹¶å°†æ–°å›¾åƒæ·»åŠ åˆ°ç‰¹å¾åº“"""
    if not input_image_path:
        return "è¯·ä¸Šä¼ å›¾ç‰‡"

    try:
        # åŠ è½½ç‰¹å¾åº“
        feature_library = load_feature_library()

        # å¼€å§‹è®¡æ—¶
        t1 = time.time()

        # é¢„å¤„ç†å›¾åƒ
        image = preprocess(Image.open(input_image_path)).unsqueeze(0).to(device)

        # æå–å›¾åƒç‰¹å¾
        with torch.no_grad():
            image_features = model.encode_image(image).float()

            # è®¡ç®—ä¸ç±»åˆ«æ–‡æœ¬çš„ç›¸ä¼¼åº¦
            logits_per_image, _ = model(image, texts)
            probs = logits_per_image.softmax(dim=-1).cpu().detach().numpy()

        # è·å–æœ€å¯èƒ½çš„ç±»åˆ«
        max_probs_index = probs.argmax(axis=1)[0]
        confidence = probs[0, max_probs_index] * 100

        # ç»“æŸè®¡æ—¶
        t2 = time.time()
        inference_time = t2 - t1

        # è¾“å‡ºç»“æœ
        result_text = f"åˆ†ç±»ç»“æœ: {word_list[max_probs_index]} ({confidence:.2f}%)"
        print(f"{result_text}ï¼Œæ¨ç†æ—¶é—´: {inference_time:.2f}ç§’")

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°å›¾åƒï¼ˆé¿å…é‡å¤æ·»åŠ ï¼‰
        is_new_image = not any(torch.allclose(image_features, existing_feature) for existing_feature in feature_library)

        if is_new_image:
            # ä¿å­˜å›¾åƒåˆ°åº“ä¸­
            output_image_idx = len(feature_library)
            output_image_path = os.path.join(output_image_folder, f"image_{output_image_idx}.jpg")
            shutil.copy(input_image_path, output_image_path)

            # æ›´æ–°ç‰¹å¾åº“
            feature_library = torch.cat((feature_library, image_features), dim=0)
            save_feature_library(feature_library)

            result_text += "\nï¼ˆå·²æ·»åŠ åˆ°å›¾ç‰‡åº“ï¼‰"
        else:
            result_text += "\nï¼ˆå›¾ç‰‡å·²å­˜åœ¨äºåº“ä¸­ï¼‰"

        return result_text

    except Exception as e:
        print(f"åˆ†ç±»è¿‡ç¨‹å‡ºé”™: {e}")
        return f"é”™è¯¯: {str(e)}"


def search_image(word):
    """æ ¹æ®å…³é”®è¯æœç´¢æœ€åŒ¹é…çš„å›¾åƒ"""
    if not word:
        return None, "è¯·è¾“å…¥æœç´¢å…³é”®è¯"

    try:
        # åŠ è½½ç‰¹å¾åº“
        feature_library = load_feature_library()

        if len(feature_library) == 0:
            return None, "å›¾ç‰‡åº“ä¸ºç©ºï¼Œè¯·å…ˆåˆ†ç±»ä¸€äº›å›¾ç‰‡"

        # ç¼–ç æœç´¢æ–‡æœ¬
        text = clip.tokenize([word]).to(device)

        with torch.no_grad():
            text_features = model.encode_text(text).float()

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = (feature_library @ text_features.T).squeeze().softmax(dim=-1)*100.0
        best_match_idx = similarities.argmax().item()
        confidence = similarities[best_match_idx].item()

        # è·å–æœ€ä½³åŒ¹é…çš„å›¾ç‰‡è·¯å¾„
        best_image_path = os.path.join(output_image_folder, f"image_{best_match_idx}.jpg")

        # ç¡®ä¿å›¾ç‰‡å­˜åœ¨
        if not os.path.exists(best_image_path):
            return None, f"æœªæ‰¾åˆ°åŒ¹é…çš„å›¾ç‰‡ï¼ˆç´¢å¼•é”™è¯¯: {best_match_idx}ï¼‰"

        print(f"æœ€ä½³åŒ¹é…: {best_image_path}ï¼Œç›¸ä¼¼åº¦: {confidence:.2f}%")
        return best_image_path, f"æœç´¢ç»“æœ: {word}ï¼ˆç›¸ä¼¼åº¦: {confidence:.2f}%ï¼‰"

    except Exception as e:
        print(f"æœç´¢è¿‡ç¨‹å‡ºé”™: {e}")
        return None, f"é”™è¯¯: {str(e)}"


def get_image_library_preview():
    """è·å–å›¾ç‰‡åº“é¢„è§ˆ"""
    if not os.path.exists(output_image_folder):
        return []

    image_files = [f for f in os.listdir(output_image_folder)
                   if f.endswith(('.jpg', '.jpeg', '.png'))]

    return [os.path.join(output_image_folder, f) for f in image_files][-12:]  # å–æœ€è¿‘çš„12å¼ å›¾ç‰‡


# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="å›¾åƒåˆ†ç±»ä¸æœç´¢åº”ç”¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("## å›¾åƒåˆ†ç±»ä¸æœç´¢åº”ç”¨")

    with gr.Row():
        # å·¦ä¾§ï¼šå›¾åƒåˆ†ç±»åŠŸèƒ½
        with gr.Column(scale=1):
            gr.Markdown("### å›¾åƒåˆ†ç±»")
            with gr.Group():
                upload = gr.Image(label="ä¸Šä¼ å›¾ç‰‡", type="filepath")
                classify_btn = gr.Button("ğŸ” å¼€å§‹åˆ†ç±»", variant="primary")
                classify_output = gr.Textbox(label="åˆ†ç±»ç»“æœ", lines=3)

        # å³ä¾§ï¼šå›¾åƒæœç´¢åŠŸèƒ½
        with gr.Column(scale=1):
            gr.Markdown("### å›¾åƒæœç´¢")
            with gr.Group():
                category_input = gr.Textbox(label="æœç´¢å…³é”®è¯",
                                            placeholder="è¾“å…¥å…³é”®è¯ï¼Œå¦‚: çŒ«ã€é¸Ÿ...")
                search_btn = gr.Button("ğŸ” æœç´¢å›¾ç‰‡", variant="primary")
                with gr.Row():
                    with gr.Column(scale=3):
                        image_output = gr.Image(label="æœç´¢ç»“æœ", type="filepath")
                    with gr.Column(scale=1):
                        search_output = gr.Textbox(label="", lines=2)

    # å›¾ç‰‡åº“é¢„è§ˆ
    gr.Markdown("### å›¾ç‰‡åº“é¢„è§ˆ")
    gallery = gr.Gallery(label="å›¾ç‰‡åº“", columns=[4, 4, 4, 4],
                         height=200, show_label=False)

    # è®¾ç½®äº‹ä»¶å¤„ç†
    classify_btn.click(
        classify_image,
        inputs=upload,
        outputs=classify_output
    )

    search_btn.click(
        search_image,
        inputs=category_input,
        outputs=[image_output, search_output]
    )

    # åº”ç”¨åŠ è½½æ—¶æ›´æ–°å›¾ç‰‡åº“é¢„è§ˆ
    demo.load(
        get_image_library_preview,
        inputs=None,
        outputs=gallery
    )

    # åˆ†ç±»åæ›´æ–°å›¾ç‰‡åº“é¢„è§ˆ
    classify_btn.click(
        get_image_library_preview,
        inputs=None,
        outputs=gallery
    )

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    demo.launch(server_name='0.0.0.0', server_port=7870, inbrowser=True)