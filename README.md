# CLIP 模型应用项目（Application with CLIP）

## 一、项目背景
本项目源于本人实习期间的相册功能复现工作，后续进一步完善后，用于数据挖掘课程的大作业，旨在探索 CLIP 模型在图像相关任务中的实际应用价值与性能表现。

## 二、项目核心内容
本项目以CLIP（Contrastive Language-Image Pre-training，对比语言 - 图像预训练）模型为核心，主要完成以下三部分工作：
1. 模型本地化部署与性能测试
- 测试数据：采用 ImageNet2012 数据集的验证集，经数据预处理后转换为 CLIP 模型要求的目标输入格式。
- 测试环境：在租借的服务器上开展实验。
- 测试模型：选取两个主流 CLIP 预训练模型进行对比测试：
  - ViT-B/32（Vision Transformer-Base/32）
  - RN50_X64（ResNet-50 X64）
- 测试结果：获取两个模型在验证集上的关键性能指标：
  - 分类准确率（TOP1 精度、TOP5 精度）
  - 推理效率（平均耗时）
2. 模型应用拓展
基于已部署的 CLIP 模型，开发两类实用功能，并完成可视化演示：
- 图像分类功能：利用模型对图像的特征提取能力，实现对输入图像的自动分类。
- 文搜图功能：建立图片的特征向量库，通过输入文本描述，检索出与文本语义匹配的图像。
## 三、补充说明：RKNN 相关内容
项目中涉及的RKNN，是适用于带算力开发板的一种架构（类似 PyTorch 中使用 GPU 进行加速），目前该技术仍需完善，使用时需注意以下要点：
- 需自行查找官方开发文档，了解详细部署流程。
- 需进行模型文件格式转换（如从.onnx 格式转为.rknn 格式）。
格式转换的关键前提：初始的.onnx 模型必须是静态图；若为动态图，需先进行模型裁剪处理，确保符合 RKNN 的输入要求。

最后，项目完整代码可从以下链接下载：（其中附训练测试运行效果和UI演示效果）
- 链接: https://pan.baidu.com/s/1MjNHMVMNrmw1aIWU6pwcmQ?pwd=t376 提取码: t376
